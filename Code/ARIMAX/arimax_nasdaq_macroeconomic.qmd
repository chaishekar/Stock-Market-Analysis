---
title: "ARIMAX/SARIMAX Model for NASDAQ Composite index and macroeconomic factors"
editor: visual
format:
  html:
    code-fold: true
    self-contained: true
    page-layout: full
---

```{r ,echo=FALSE, message=FALSE, warning=FALSE}
library(flipbookr)
library(tidyverse)
library(ggplot2)
library(forecast)
library(astsa) 
library(xts)
library(tseries)
library(fpp2)
library(fma)
library(lubridate)
library(tidyverse)
library(TSstudio)
library(quantmod)
library(tidyquant)
library(plotly)
library(ggplot2)
library(reshape2)
require(gridExtra)
library(magrittr)
library(vars)
options(dplyr.summarise.inform = FALSE)
```

```{r,echo=FALSE, message=FALSE, warning=FALSE}
#import the data
dj_data <- read.csv("DATA/CLEANED DATA/dji_raw_data.csv")
nasdaq_data <- read.csv("DATA/CLEANED DATA/nasdaq_raw_data.csv")
sp500_data <- read.csv("DATA/CLEANED DATA/sp500_raw_data.csv")
xlb_data <- read.csv("DATA/CLEANED DATA/xlb_raw_data.csv")
xlc_data <- read.csv("DATA/CLEANED DATA/xlc_raw_data.csv")
xle_data <- read.csv("DATA/CLEANED DATA/xle_raw_data.csv")
xlf_data <- read.csv("DATA/CLEANED DATA/xlf_raw_data.csv")
xli_data <- read.csv("DATA/CLEANED DATA/xli_raw_data.csv")
xlp_data <- read.csv("DATA/CLEANED DATA/xlp_raw_data.csv")
xlk_data <- read.csv("DATA/CLEANED DATA/xlk_raw_data.csv")
xlre_data <- read.csv("DATA/CLEANED DATA/xlre_raw_data.csv")
xlu_data <- read.csv("DATA/CLEANED DATA/xlu_raw_data.csv")
xlv_data <- read.csv("DATA/CLEANED DATA/xlv_raw_data.csv")
xly_data <- read.csv("DATA/CLEANED DATA/xly_raw_data.csv")
gdp_data <- read.csv("DATA/RAW DATA/gdp-growth.csv")
interest_data <- read.csv("DATA/RAW DATA/interest-rate.csv")
inflation_data <- read.csv("DATA/CLEANED DATA/inflation_yearly_data.csv")
unemployment_data <- read.csv("DATA/RAW DATA/unemployment-rate.csv")

#clean data
dj_data_clean <-dj_data %>% dplyr::select(Date, DJI.Adjusted)
nasdaq_data_clean <- nasdaq_data %>%dplyr::select(Date, IXIC.Adjusted)
sp500_data_clean <- sp500_data%>%dplyr::select( Date, GSPC.Adjusted)
xlb_data_clean <- xlb_data%>%dplyr::select(Date,XLB.Adjusted )
xlc_data_clean <- xlc_data%>%dplyr::select(Date, XLC.Adjusted)
xle_data_clean <- xle_data%>%dplyr::select( Date, XLE.Adjusted)
xlf_data_clean <- xlf_data%>%dplyr::select( Date, XLF.Adjusted)
xli_data_clean <- xli_data%>%dplyr::select( Date, XLI.Adjusted)
xlp_data_clean <- xlp_data%>%dplyr::select( Date, XLP.Adjusted)
xlk_data_clean <- xlk_data%>%dplyr::select( Date, XLK.Adjusted)
xlre_data_clean <- xlre_data%>%dplyr::select( Date, XLRE.Adjusted)
xlu_data_clean <- xlu_data%>%dplyr::select( Date, XLU.Adjusted)
xlv_data_clean <- xlv_data%>%dplyr::select( Date, XLV.Adjusted)
xly_data_clean <- xly_data%>%dplyr::select( Date, XLY.Adjusted)

# changing the data to quarterly based data
#dj_data_clean
dj_data_clean$Date<-as.Date(dj_data_clean$Date,"%m/%d/%Y")
dj_data_clean <- dj_data_clean %>%
  mutate(Q = cut.Date(Date, "quarter", labels = FALSE)) %>%
  group_by(Q) %>%
  filter(Date == min(Date)) %>%
  mutate(Date = as.Date(paste0(year(Date), "-", sprintf("%02d", month(Date)), "-01")))
drop <- c("Q")
dj_data_clean = dj_data_clean[,!(names(dj_data_clean) %in% drop)]
#nasdaq_data_clean
nasdaq_data_clean$Date<-as.Date(nasdaq_data_clean$Date,"%m/%d/%Y")
nasdaq_data_clean <- nasdaq_data_clean %>%
  mutate(Q = cut.Date(Date, "quarter", labels = FALSE)) %>%
  group_by(Q) %>%
  filter(Date == min(Date)) %>%
  mutate(Date = as.Date(paste0(year(Date), "-", sprintf("%02d", month(Date)), "-01")))
drop <- c("Q")
nasdaq_data_clean = nasdaq_data_clean[,!(names(nasdaq_data_clean) %in% drop)]
#sp500_data_clean
sp500_data_clean$Date<-as.Date(sp500_data_clean$Date,"%m/%d/%Y")
sp500_data_clean <- sp500_data_clean %>%
  mutate(Q = cut.Date(Date, "quarter", labels = FALSE)) %>%
  group_by(Q) %>%
  filter(Date == min(Date)) %>%
  mutate(Date = as.Date(paste0(year(Date), "-", sprintf("%02d", month(Date)), "-01")))
drop <- c("Q")
sp500_data_clean = sp500_data_clean[,!(names(sp500_data_clean) %in% drop)]
#xlb_data_clean
xlb_data_clean$Date<-as.Date(xlb_data_clean$Date,"%m/%d/%Y")
xlb_data_clean <- xlb_data_clean %>%
  mutate(Q = cut.Date(Date, "quarter", labels = FALSE)) %>%
  group_by(Q) %>%
  filter(Date == min(Date)) %>%
  mutate(Date = as.Date(paste0(year(Date), "-", sprintf("%02d", month(Date)), "-01")))
drop <- c("Q")
xlb_data_clean = xlb_data_clean[,!(names(xlb_data_clean) %in% drop)]
#xlc_data_clean
xlc_data_clean$Date<-as.Date(xlc_data_clean$Date,"%m/%d/%Y")
xlc_data_clean <- xlc_data_clean %>%
  mutate(Q = cut.Date(Date, "quarter", labels = FALSE)) %>%
  group_by(Q) %>%
  filter(Date == min(Date)) %>%
  mutate(Date = as.Date(paste0(year(Date), "-", sprintf("%02d", month(Date)), "-01")))
drop <- c("Q")
xlc_data_clean = xlc_data_clean[,!(names(xlc_data_clean) %in% drop)]
#xle_data_clean
xle_data_clean$Date<-as.Date(xle_data_clean$Date,"%m/%d/%Y")
xle_data_clean <- xle_data_clean %>%
  mutate(Q = cut.Date(Date, "quarter", labels = FALSE)) %>%
  group_by(Q) %>%
  filter(Date == min(Date)) %>%
  mutate(Date = as.Date(paste0(year(Date), "-", sprintf("%02d", month(Date)), "-01")))
drop <- c("Q")
xle_data_clean = xle_data_clean[,!(names(xle_data_clean) %in% drop)]
#xlf_data_clean 
xlf_data_clean$Date<-as.Date(xlf_data_clean$Date,"%m/%d/%Y")
xlf_data_clean <- xlf_data_clean %>%
  mutate(Q = cut.Date(Date, "quarter", labels = FALSE)) %>%
  group_by(Q) %>%
  filter(Date == min(Date)) %>%
  mutate(Date = as.Date(paste0(year(Date), "-", sprintf("%02d", month(Date)), "-01")))
drop <- c("Q")
xlf_data_clean = xlf_data_clean[,!(names(xlf_data_clean) %in% drop)]
#xli_data_clean 
xli_data_clean$Date<-as.Date(xli_data_clean$Date,"%m/%d/%Y")
xli_data_clean <- xli_data_clean %>%
  mutate(Q = cut.Date(Date, "quarter", labels = FALSE)) %>%
  group_by(Q) %>%
  filter(Date == min(Date)) %>%
  mutate(Date = as.Date(paste0(year(Date), "-", sprintf("%02d", month(Date)), "-01")))
drop <- c("Q")
xli_data_clean = xli_data_clean[,!(names(xli_data_clean) %in% drop)]
#xlp_data_clean 
xlp_data_clean$Date<-as.Date(xlp_data_clean$Date,"%m/%d/%Y")
xlp_data_clean <- xlp_data_clean %>%
  mutate(Q = cut.Date(Date, "quarter", labels = FALSE)) %>%
  group_by(Q) %>%
  filter(Date == min(Date)) %>%
  mutate(Date = as.Date(paste0(year(Date), "-", sprintf("%02d", month(Date)), "-01")))
drop <- c("Q")
xlp_data_clean = xlp_data_clean[,!(names(xlp_data_clean) %in% drop)]
#xlk_data_clean
xlk_data_clean$Date<-as.Date(xlk_data_clean$Date,"%m/%d/%Y")
xlk_data_clean <- xlk_data_clean %>%
  mutate(Q = cut.Date(Date, "quarter", labels = FALSE)) %>%
  group_by(Q) %>%
  filter(Date == min(Date)) %>%
  mutate(Date = as.Date(paste0(year(Date), "-", sprintf("%02d", month(Date)), "-01")))
drop <- c("Q")
xlk_data_clean = xlk_data_clean[,!(names(xlk_data_clean) %in% drop)]
#xlre_data_clean 
xlre_data_clean$Date<-as.Date(xlre_data_clean$Date,"%m/%d/%Y")
xlre_data_clean <- xlre_data_clean %>%
  mutate(Q = cut.Date(Date, "quarter", labels = FALSE)) %>%
  group_by(Q) %>%
  filter(Date == min(Date)) %>%
  mutate(Date = as.Date(paste0(year(Date), "-", sprintf("%02d", month(Date)), "-01")))
drop <- c("Q")
xlre_data_clean = xlre_data_clean[,!(names(xlre_data_clean) %in% drop)]
#xlu_data_clean 
xlu_data_clean$Date<-as.Date(xlu_data_clean$Date,"%m/%d/%Y")
xlu_data_clean <- xlu_data_clean %>%
  mutate(Q = cut.Date(Date, "quarter", labels = FALSE)) %>%
  group_by(Q) %>%
  filter(Date == min(Date)) %>%
  mutate(Date = as.Date(paste0(year(Date), "-", sprintf("%02d", month(Date)), "-01")))
drop <- c("Q")
xlu_data_clean = xlu_data_clean[,!(names(xlu_data_clean) %in% drop)]
#xlv_data_clean 
xlv_data_clean$Date<-as.Date(xlv_data_clean$Date,"%m/%d/%Y")
xlv_data_clean <- xlv_data_clean %>%
  mutate(Q = cut.Date(Date, "quarter", labels = FALSE)) %>%
  group_by(Q) %>%
  filter(Date == min(Date)) %>%
  mutate(Date = as.Date(paste0(year(Date), "-", sprintf("%02d", month(Date)), "-01")))
drop <- c("Q")
xlv_data_clean = xlv_data_clean[,!(names(xlv_data_clean) %in% drop)]
#xly_data_clean 
xly_data_clean$Date<-as.Date(xly_data_clean$Date,"%m/%d/%Y")
xly_data_clean <- xly_data_clean %>%
  mutate(Q = cut.Date(Date, "quarter", labels = FALSE)) %>%
  group_by(Q) %>%
  filter(Date == min(Date)) %>%
  mutate(Date = as.Date(paste0(year(Date), "-", sprintf("%02d", month(Date)), "-01")))
drop <- c("Q")
xly_data_clean = xly_data_clean[,!(names(xly_data_clean) %in% drop)]
#interest_data 
interest_data$Date<-as.Date(interest_data$Date,"%m/%d/%Y")
interest_data <- interest_data %>%
  mutate(Q = cut.Date(Date, "quarter", labels = FALSE)) %>%
  group_by(Q) %>%
  filter(Date == min(Date)) %>%
  mutate(Date = as.Date(paste0(year(Date), "-", sprintf("%02d", month(Date)), "-01")))
colnames(interest_data)[2] ="interest"
drop <- c("Q")
interest_data = interest_data[,!(names(interest_data) %in% drop)]
#inflation_data
inflation_data$Date<-as.Date(inflation_data$Date,"%m/%d/%Y")
inflation_data <- inflation_data %>%
  mutate(Q = cut.Date(Date, "quarter", labels = FALSE)) %>%
  group_by(Q) %>%
  filter(Date == min(Date)) %>%
  mutate(Date = as.Date(paste0(year(Date), "-", sprintf("%02d", month(Date)), "-01")))
colnames(inflation_data)[2] ="inflation"
drop <- c("Q")
inflation_data = inflation_data[,!(names(inflation_data) %in% drop)]
#unemployment_data 
unemployment_data$Date<-as.Date(unemployment_data$Date,"%m/%d/%Y")
unemployment_data <- unemployment_data %>%
  mutate(Q = cut.Date(Date, "quarter", labels = FALSE)) %>%
  group_by(Q) %>%
  filter(Date == min(Date)) %>%
  mutate(Date = as.Date(paste0(year(Date), "-", sprintf("%02d", month(Date)), "-01")))
colnames(unemployment_data)[2] ="unemployment"
drop <- c("Q")
unemployment_data = unemployment_data[,!(names(unemployment_data) %in% drop)]
#gdp_data
gdp_data$Date <- as.Date(gdp_data$DATE , "%m/%d/%Y")
#drop DATE column from gdp_data
gdp_data <- gdp_data %>%dplyr::select( Date, value)
colnames(gdp_data)[2] ="gdp"

drop <- c("Q")
df = unemployment_data[,!(names(unemployment_data) %in% drop)]

# final merged data
df_index_factor <- list(dj_data_clean,nasdaq_data_clean, sp500_data_clean,gdp_data, interest_data, inflation_data, unemployment_data)      
index_factor_data <- Reduce(function(x, y) merge(x, y, all=TRUE), df_index_factor) 
index_factor_data <- na.omit(index_factor_data)

df_index_sector <- list(dj_data_clean, nasdaq_data_clean, sp500_data_clean, xlb_data_clean, xlc_data_clean, xle_data_clean, xlf_data_clean, xli_data_clean, xlp_data_clean, xlk_data_clean, xlre_data_clean, xlu_data_clean, xlv_data_clean, xly_data_clean)
index_sector_data <- Reduce(function(x, y) merge(x, y, all=TRUE), df_index_sector) 
index_sector_data <- na.omit(index_sector_data)

df_sector_factor <- list(xlb_data_clean, xlc_data_clean, xle_data_clean, xlf_data_clean, xli_data_clean, xlp_data_clean, xlk_data_clean, xlre_data_clean, xlu_data_clean, xlv_data_clean, xly_data_clean,gdp_data, interest_data, inflation_data, unemployment_data )
sector_factor_data <- Reduce(function(x, y) merge(x, y, all=TRUE), df_sector_factor) 
sector_factor_data <- na.omit(sector_factor_data)

numeric_vars_index_factor_data <- c("DJI.Adjusted", "IXIC.Adjusted", "GSPC.Adjusted", "gdp", "interest", "inflation", "unemployment")
numeric_index_factor_data <- index_factor_data[, numeric_vars_index_factor_data]
normalized_index_factor_data_numeric <- scale(numeric_index_factor_data)
normalized_index_factor_data <- ts(normalized_index_factor_data_numeric, start = c(2010, 1), frequency = 4)
```

Endogenous variable: NASDAQ Composite Index Exogenous variables: Macroeconomic indicators such as GDP, inflation rate, unemployment rate and interest rates

Before proceeding with model selection, it can be useful to examine the feature variables and select the appropriate ones. This can be done by creating cross-correlation function plots, which can serve as a verification of the correlation heatmap for the variables.

##### CCF Plot for NASDAQ Composite and Exogenous Variables

::: panel-tabset
### GDP

```{r}
par(mfrow=c(1,1))
ccf_result <- ccf(normalized_index_factor_data[, c("IXIC.Adjusted")], normalized_index_factor_data[, c("gdp")], 
    lag.max = 300,
    main = "Cros-Correlation Plot for NASDAQ Composite Index and GDP Growth Rate ",
    ylab = "CCF")

cat("The sum of cross correlation function is", sum(abs(ccf_result$acf)))
```

### Interest Rate

```{r}
par(mfrow=c(1,1))
ccf_result <- ccf(normalized_index_factor_data[, c("IXIC.Adjusted")], normalized_index_factor_data[, c("interest")], 
    lag.max = 300,
    main = "Cros-Correlation Plot for NASDAQ Composite Index and Interest Rate",
    ylab = "CCF")

cat("The sum of cross correlation function is", sum(abs(ccf_result$acf)))
```

### Inflation Rate

```{r}
par(mfrow=c(1,1))
ccf_result <- ccf(normalized_index_factor_data[, c("IXIC.Adjusted")], normalized_index_factor_data[, c("inflation")], 
    lag.max = 300,
    main = "Cros-Correlation Plot for NASDAQ Composite Index and Inflation Rate",
    ylab = "CCF")

cat("The sum of cross correlation function is", sum(abs(ccf_result$acf)))
```

### Unemployment Rate

```{r}
par(mfrow=c(1,1))
ccf_result <- ccf(normalized_index_factor_data[, c("IXIC.Adjusted")], normalized_index_factor_data[, c("unemployment")], 
    lag.max = 300,
    main = "Cros-Correlation Plot for NASDAQ Composite Index and Unemployment Rate",
    ylab = "CCF")

cat("The sum of cross correlation function is", sum(abs(ccf_result$acf)))
```
:::

The cross-correlation feature plots reaffirm the findings from the heatmap analysis, indicating that inflation and unemployment rate exhibit stronger correlations with the index compared to GDP and interest rates. The cross-correlation plots for GDP and interest rates show weaker and more scattered patterns, with strikes between the blue lines indicating lower correlation coefficients. This suggests that inflation and unemployment rate are more suitable feature variables for the ARIMAX model when predicting NASDAQ Composite movements.

Final Exogenous variables: Macroeconomic indicators: Inflation rate and unemployment rate.

##### Endogenous and Exogenous Variables Plot

```{r}
final_nadsaq_factor_data <- index_factor_data %>%dplyr::select( Date,IXIC.Adjusted, inflation,unemployment)
numeric_vars_nadsaq_factor_data <- c("IXIC.Adjusted", "inflation", "unemployment")
numeric_nadsaq_factor_data <- final_nadsaq_factor_data[, numeric_vars_nadsaq_factor_data]
normalized_nadsaq_factor_data_numeric <- scale(numeric_nadsaq_factor_data)
normalized_nadsaq_factor_data_numeric_df <- data.frame(normalized_nadsaq_factor_data_numeric)
normalized_nadsaq_factor_data_ts <- ts(normalized_nadsaq_factor_data_numeric, start = c(2010, 1), frequency = 4)

autoplot(normalized_nadsaq_factor_data_ts, facets=TRUE) +
  xlab("Year") + ylab("") +
  ggtitle("NASDAQ Composite Index Stock Price, Inflation Rate and Unemployment Rate in USA 2010-2023")
```

##### Check the stationarity

Before proceeding with further analysis, it's important to check the stationarity of the time series data. If the series is not stationary, it needs to be differentiated before performing ARIMAX model. The stationarity of a multivariate time series data can be checked using the Phillips-Perron test, which is specifically designed for multivariate data. By performing the Phillips-Perron test on the multivariate time series data, we can determine if the data is stationary or if it requires differencing to make it suitable for ARIMAX modeling.

```{r}
# Convert your multivariate time series data to a matrix
final_nadsaq_factor_data_ts_multivariate <- as.matrix(normalized_nadsaq_factor_data_ts)

# Check for stationarity using Phillips-Perron test
phillips_perron_test <- ur.pp(final_nadsaq_factor_data_ts_multivariate)  
summary(phillips_perron_test)
```

The Phillips-Perron unit root test output suggests that the data is stationary. The test regression includes an intercept term and tests whether the coefficient on the lagged value is significantly different from one. The estimated coefficients indicate that the intercept is not significant (p-value \> 0.05), while the coefficient on the lagged value is highly significant (p-value \< 2e-16), suggesting that the data is stationary.

Additionally, the "Z-alpha" value is the test statistic, which is compared to critical values of the standard normal distribution to determine significance. In this case, the test statistic is -23.0718, indicating strong evidence against the null hypothesis of a unit root. The "Z-tau-mu" value is an auxiliary test statistic used to correct for autocorrelation in the residuals, and it is close to zero, suggesting that there is no evidence of residual autocorrelation.

##### Fitting a ARIMAX model

Fitting an ARIMAX model will be a useful approach to modeling and predicting time series data when external factors are believed to have an impact on the response variable. In this case, we will be using an ARIMAX model to predict the NASDAQ Composite Index, taking into account the effects of inflation and unemployment on the Index. By including these exogenous variables in our model, we can improve the accuracy of our predictions and gain a better understanding of the underlying dynamics of the time series.

::: panel-tabset
### Auto ARIMA

```{r}
xreg <- cbind(Inflation = normalized_nadsaq_factor_data_ts[, "inflation"],
              Unemployment = normalized_nadsaq_factor_data_ts[, "unemployment"])

fit <- auto.arima(normalized_nadsaq_factor_data_ts[, "IXIC.Adjusted"], xreg = xreg)
summary(fit)
```

### Residuals

```{r}
checkresiduals(fit)
```
:::

The auto.arima function reveals that the best-fit model is (0,1,0). Analysis of the residuals plot indicates that the fluctuates a lot and lies between -2000 to 2000.The ACF plot of the residuals shows significant lags, indicating that the model is performing well. The qq-plot also suggests normality in the residuals. The p-value greater than 0.05 actually suggests that there is no evidence of significant autocorrelation in the residuals, which means that the model is performing well in terms of capturing the temporal patterns in the data.

##### Fitting the model manually

::: panel-tabset
### Linear Model

```{r}
normalized_nadsaq_factor_data_numeric_df$IXIC.Adjusted<-ts(normalized_nadsaq_factor_data_numeric_df$IXIC.Adjusted,star=decimal_date(as.Date("2010-01-01",format = "%Y-%m-%d")),frequency = 4)
normalized_nadsaq_factor_data_numeric_df$inflation<-ts(normalized_nadsaq_factor_data_numeric_df$inflation,star=decimal_date(as.Date("2010-01-01",format = "%Y-%m-%d")),frequency = 4)
normalized_nadsaq_factor_data_numeric_df$unemployment<-ts(normalized_nadsaq_factor_data_numeric_df$unemployment,star=decimal_date(as.Date("2010-01-01",format = "%Y-%m-%d")),frequency = 4)

############# First fit the linear model##########
fit.reg <- lm(IXIC.Adjusted ~ inflation+unemployment, data=normalized_nadsaq_factor_data_numeric_df)
summary(fit.reg)
```

### ACF of Residuals

```{r}
res.fit<-ts(residuals(fit.reg),star=decimal_date(as.Date("2010-01-01",format = "%Y-%m-%d")),frequency = 4)
############## Then look at the residuals ############
acf(res.fit)

```

### ACF of Residuals

```{r}
Pacf(res.fit)
```

### Differentiated Residual

```{r}
res.fit %>% diff() %>% ggtsdisplay()

```
:::

The output displays the results of a linear regression model using two predictor variables, inflation and unemployment, to explain the variation in the response variable, IXIC.Adjusted. The coefficients of inflation and unemployment have p-values of less than 0.05, indicating that both variables significantly impact IXIC.Adjusted. The R-squared value of approximately 52% suggests that the model explains a moderate amount of the variation in the response variable. Examination of the ACF plot of residuals reveals some autocorrelation, and therefore, the series is differentiated to achieve stationarity. By analyzing the differentiated ACF and PACF plots, the parameters if the ARIMAX model are that p=1, q=1,2, and d=1.

##### Finding the model parameters.

::: panel-tabset
### ARIMAX Result

```{r}
ARIMA.c=function(p1,q1,q2,data){
temp=c()
d=1
i=1
temp= data.frame()
ls=matrix(rep(NA,6*30),nrow=30)


for (p in p1)#
{
  for(q in q1:q2)#
  {
    for(d in 0:1)
    {
      
      if(p+d+q<=6)
      {
        
        model<- Arima(data,order=c(p,d,q))
        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)
        i=i+1
        #print(i)
        
      }
      
    }
  }
}


temp= as.data.frame(ls)
names(temp)= c("p","d","q","AIC","BIC","AICc")

temp
}

output <- ARIMA.c(1,1,2,data=residuals(fit.reg))

output[which.min(output$AIC),] 
output[which.min(output$BIC),]
output[which.min(output$AICc),]
```

### Model 1 Plot

```{r}
set.seed(1234)

model_output_1 <- capture.output(sarima(res.fit, 1,1,1)) 
```

### Model 1

```{r}
cat(model_output_1[32:63], model_output_1[length(model_output_1)], sep = "\n")
```

### Model 2 Plot

```{r}
model_output_2 <- capture.output(sarima(res.fit,1,1,2)) 
```

### Model 2

```{r}
cat(model_output_2[31:63], model_output_2[length(model_output_2)], sep = "\n")
```

### Model 3 Plot

```{r}
model_output_3 <- capture.output(sarima(res.fit,0,1,0)) 
```

### Model 3

```{r}
cat(model_output_3[9:38], model_output_3[length(model_output_3)], sep = "\n")
```
:::

Following the manual fitting process, we identified that the ARIMAX model with the minimum AIC and AICc has the parameters (1,1,2), while the model with the minimum BIC has the parameters (1,1,1). We fitted both models using both the manual and auto.arima methods and observed that all the models have p-values greater than 0, indicating statistical significance. To determine the best fit model, we performed cross-validation.

##### Cross Validation

::: panel-tabset
### RMSE Plot

```{r}
n=length(res.fit)
k= 51
 
 
rmse1 <- matrix(NA, (n-k),4)
rmse2 <- matrix(NA, (n-k),4)
rmse3 <- matrix(NA, (n-k),4)


st <- tsp(res.fit)[1]+(k-5)/4 

for(i in 1:(n-k))
{
  xtrain <- window(res.fit, end=st + i/4)
  xtest <- window(res.fit, start=st + (i+1)/4, end=st + (i+4)/4)
  
  #ARIMA(0,1,0) ARIMA(1,1,1) ARIMA(1,1,2)
  
  fit <- Arima(xtrain, order=c(1,1,2),
                include.drift=TRUE, method="ML")
  fcast <- forecast(fit, h=4)
  
  fit2 <- Arima(xtrain, order=c(1,1,1),
                include.drift=TRUE, method="ML")
  fcast2 <- forecast(fit2, h=4)

  fit3 <- Arima(xtrain, order=c(0,1,0),
                include.drift=TRUE, method="ML")
  fcast3 <- forecast(fit3, h=4)

  rmse1[i,1:length(xtest)]   <- sqrt((fcast$mean-xtest)^2)
  rmse2[i,1:length(xtest)] <- sqrt((fcast2$mean-xtest)^2)
  rmse3[i,1:length(xtest)] <- sqrt((fcast3$mean-xtest)^2)
}

plot(1:4,colMeans(rmse1,na.rm=TRUE), type="l",col=2, xlab="horizon", ylab="RMSE")
lines(1:4, colMeans(rmse2,na.rm=TRUE), type="l",col=3)
lines(1:4, colMeans(rmse3,na.rm=TRUE), type="l",col=4)
legend("topleft",legend=c("fit1","fit2","fit3"),col=2:4,lty=1)
```

### RMSE Results

```{r}
cat("RMSE values for Model 1\n", colMeans(rmse1))

cat("RMSE values for Model 2\n", colMeans(rmse2))

cat("RMSE values for Model 3\n", colMeans(rmse3))
```
:::

The RMSE cross-validation plot shows that the RMSE of Model 1 (1,1,2) is lower than that of Model 2 (1,1,1) and Model 3 (0,1,0). This suggests that Model 1 (1,1,2) is a better fit for the data when compared other models.

##### Forecast

::: panel-tabset
### Forecast for NASDAQ Index with feature variables

```{r}
#fiting an ARIMA model to the Inflation variable
inflation_fit<-auto.arima(normalized_nadsaq_factor_data_numeric_df$inflation) 
finflation<-forecast(inflation_fit)

#fitting an ARIMA model to the Unemployment variable
unemployment_fit<-auto.arima(normalized_nadsaq_factor_data_numeric_df$unemployment) 
funemployment<-forecast(unemployment_fit)

# best model fit for forcasting
xreg <- cbind(Inflation = normalized_nadsaq_factor_data_ts[, "inflation"],
              Unemployment = normalized_nadsaq_factor_data_ts[, "unemployment"])

fit <- Arima(normalized_nadsaq_factor_data_ts[, "IXIC.Adjusted"],order=c(1,1,3),xreg=xreg)

# Forcast the stock price using feature variables
fxreg <- cbind(Inflation = finflation$mean,
              Unemployment = funemployment$mean)
fcast <- forecast(fit, xreg=fxreg) 
autoplot(fcast) + xlab("Year") +
  ylab("Normalised IXIC.Adjusted")
```

### ARIMA Model for Inflation

```{r}
#fiting an ARIMA model to the Inflation variable
inflation_fit<-auto.arima(normalized_nadsaq_factor_data_numeric_df$inflation) 
finflation<-forecast(inflation_fit)
summary(inflation_fit)
```

### ARIMA Model for Employment

```{r}
#fitting an ARIMA model to the Unemployment variable
unemployment_fit<-auto.arima(normalized_nadsaq_factor_data_numeric_df$unemployment) 
funemployment<-forecast(unemployment_fit)
summary(unemployment_fit)
```

### ARIMA Model for NASDAQ Composite Index with feature variables

```{r}
# best model fit for forcasting
xreg <- cbind(Inflation = normalized_nadsaq_factor_data_ts[, "inflation"],
              Unemployment = normalized_nadsaq_factor_data_ts[, "unemployment"])

fit <- Arima(normalized_nadsaq_factor_data_ts[, "IXIC.Adjusted"],order=c(1,1,3),xreg=xreg)
summary(fit)
```
:::

The ARIMAX model may have incorporated exogenous variables such as macroeconomic factors, which could indicate the potential impact of external factors on the forecasted values. The findings suggest that inflation and unemployment rates significantly impact the NADSAQ Composite Index stock price, underscoring the importance of including macroeconomic factors in stock market forecasting. This is further supported by the comparison of RMSE, MAE, AIC and BIC values for forecasts with and without the feature variable. It is found that including the feature variable leads to a lower values, indicating that the feature variable makes a significant impact and should be considered in stock market forecasting.

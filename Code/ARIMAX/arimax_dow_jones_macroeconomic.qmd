---
title: "ARIMAX/SARIMAX Model for Dow Jones index and macroeconomic factors"
editor: visual
format:
  html:
    code-fold: true
    self-contained: true
    page-layout: full
---

```{r ,echo=FALSE, message=FALSE, warning=FALSE}
library(flipbookr)
library(tidyverse)
library(ggplot2)
library(forecast)
library(astsa) 
library(xts)
library(tseries)
library(fpp2)
library(fma)
library(lubridate)
library(tidyverse)
library(TSstudio)
library(quantmod)
library(tidyquant)
library(plotly)
library(ggplot2)
library(reshape2)
require(gridExtra)
library(magrittr)
library(vars)
options(dplyr.summarise.inform = FALSE)
```

```{r,echo=FALSE, message=FALSE, warning=FALSE}
#import the data
dj_data <- read.csv("DATA/CLEANED DATA/dji_raw_data.csv")
nasdaq_data <- read.csv("DATA/CLEANED DATA/nasdaq_raw_data.csv")
sp500_data <- read.csv("DATA/CLEANED DATA/sp500_raw_data.csv")
xlb_data <- read.csv("DATA/CLEANED DATA/xlb_raw_data.csv")
xlc_data <- read.csv("DATA/CLEANED DATA/xlc_raw_data.csv")
xle_data <- read.csv("DATA/CLEANED DATA/xle_raw_data.csv")
xlf_data <- read.csv("DATA/CLEANED DATA/xlf_raw_data.csv")
xli_data <- read.csv("DATA/CLEANED DATA/xli_raw_data.csv")
xlp_data <- read.csv("DATA/CLEANED DATA/xlp_raw_data.csv")
xlk_data <- read.csv("DATA/CLEANED DATA/xlk_raw_data.csv")
xlre_data <- read.csv("DATA/CLEANED DATA/xlre_raw_data.csv")
xlu_data <- read.csv("DATA/CLEANED DATA/xlu_raw_data.csv")
xlv_data <- read.csv("DATA/CLEANED DATA/xlv_raw_data.csv")
xly_data <- read.csv("DATA/CLEANED DATA/xly_raw_data.csv")
gdp_data <- read.csv("DATA/RAW DATA/gdp-growth.csv")
interest_data <- read.csv("DATA/RAW DATA/interest-rate.csv")
inflation_data <- read.csv("DATA/CLEANED DATA/inflation_yearly_data.csv")
unemployment_data <- read.csv("DATA/RAW DATA/unemployment-rate.csv")

#clean data
dj_data_clean <-dj_data %>% dplyr::select(Date, DJI.Adjusted)
nasdaq_data_clean <- nasdaq_data %>%dplyr::select(Date, IXIC.Adjusted)
sp500_data_clean <- sp500_data%>%dplyr::select( Date, GSPC.Adjusted)
xlb_data_clean <- xlb_data%>%dplyr::select(Date,XLB.Adjusted )
xlc_data_clean <- xlc_data%>%dplyr::select(Date, XLC.Adjusted)
xle_data_clean <- xle_data%>%dplyr::select( Date, XLE.Adjusted)
xlf_data_clean <- xlf_data%>%dplyr::select( Date, XLF.Adjusted)
xli_data_clean <- xli_data%>%dplyr::select( Date, XLI.Adjusted)
xlp_data_clean <- xlp_data%>%dplyr::select( Date, XLP.Adjusted)
xlk_data_clean <- xlk_data%>%dplyr::select( Date, XLK.Adjusted)
xlre_data_clean <- xlre_data%>%dplyr::select( Date, XLRE.Adjusted)
xlu_data_clean <- xlu_data%>%dplyr::select( Date, XLU.Adjusted)
xlv_data_clean <- xlv_data%>%dplyr::select( Date, XLV.Adjusted)
xly_data_clean <- xly_data%>%dplyr::select( Date, XLY.Adjusted)

# changing the data to quarterly based data
#dj_data_clean
dj_data_clean$Date<-as.Date(dj_data_clean$Date,"%m/%d/%Y")
dj_data_clean <- dj_data_clean %>%
  mutate(Q = cut.Date(Date, "quarter", labels = FALSE)) %>%
  group_by(Q) %>%
  filter(Date == min(Date)) %>%
  mutate(Date = as.Date(paste0(year(Date), "-", sprintf("%02d", month(Date)), "-01")))
drop <- c("Q")
dj_data_clean = dj_data_clean[,!(names(dj_data_clean) %in% drop)]
#nasdaq_data_clean
nasdaq_data_clean$Date<-as.Date(nasdaq_data_clean$Date,"%m/%d/%Y")
nasdaq_data_clean <- nasdaq_data_clean %>%
  mutate(Q = cut.Date(Date, "quarter", labels = FALSE)) %>%
  group_by(Q) %>%
  filter(Date == min(Date)) %>%
  mutate(Date = as.Date(paste0(year(Date), "-", sprintf("%02d", month(Date)), "-01")))
drop <- c("Q")
nasdaq_data_clean = nasdaq_data_clean[,!(names(nasdaq_data_clean) %in% drop)]
#sp500_data_clean
sp500_data_clean$Date<-as.Date(sp500_data_clean$Date,"%m/%d/%Y")
sp500_data_clean <- sp500_data_clean %>%
  mutate(Q = cut.Date(Date, "quarter", labels = FALSE)) %>%
  group_by(Q) %>%
  filter(Date == min(Date)) %>%
  mutate(Date = as.Date(paste0(year(Date), "-", sprintf("%02d", month(Date)), "-01")))
drop <- c("Q")
sp500_data_clean = sp500_data_clean[,!(names(sp500_data_clean) %in% drop)]
#xlb_data_clean
xlb_data_clean$Date<-as.Date(xlb_data_clean$Date,"%m/%d/%Y")
xlb_data_clean <- xlb_data_clean %>%
  mutate(Q = cut.Date(Date, "quarter", labels = FALSE)) %>%
  group_by(Q) %>%
  filter(Date == min(Date)) %>%
  mutate(Date = as.Date(paste0(year(Date), "-", sprintf("%02d", month(Date)), "-01")))
drop <- c("Q")
xlb_data_clean = xlb_data_clean[,!(names(xlb_data_clean) %in% drop)]
#xlc_data_clean
xlc_data_clean$Date<-as.Date(xlc_data_clean$Date,"%m/%d/%Y")
xlc_data_clean <- xlc_data_clean %>%
  mutate(Q = cut.Date(Date, "quarter", labels = FALSE)) %>%
  group_by(Q) %>%
  filter(Date == min(Date)) %>%
  mutate(Date = as.Date(paste0(year(Date), "-", sprintf("%02d", month(Date)), "-01")))
drop <- c("Q")
xlc_data_clean = xlc_data_clean[,!(names(xlc_data_clean) %in% drop)]
#xle_data_clean
xle_data_clean$Date<-as.Date(xle_data_clean$Date,"%m/%d/%Y")
xle_data_clean <- xle_data_clean %>%
  mutate(Q = cut.Date(Date, "quarter", labels = FALSE)) %>%
  group_by(Q) %>%
  filter(Date == min(Date)) %>%
  mutate(Date = as.Date(paste0(year(Date), "-", sprintf("%02d", month(Date)), "-01")))
drop <- c("Q")
xle_data_clean = xle_data_clean[,!(names(xle_data_clean) %in% drop)]
#xlf_data_clean 
xlf_data_clean$Date<-as.Date(xlf_data_clean$Date,"%m/%d/%Y")
xlf_data_clean <- xlf_data_clean %>%
  mutate(Q = cut.Date(Date, "quarter", labels = FALSE)) %>%
  group_by(Q) %>%
  filter(Date == min(Date)) %>%
  mutate(Date = as.Date(paste0(year(Date), "-", sprintf("%02d", month(Date)), "-01")))
drop <- c("Q")
xlf_data_clean = xlf_data_clean[,!(names(xlf_data_clean) %in% drop)]
#xli_data_clean 
xli_data_clean$Date<-as.Date(xli_data_clean$Date,"%m/%d/%Y")
xli_data_clean <- xli_data_clean %>%
  mutate(Q = cut.Date(Date, "quarter", labels = FALSE)) %>%
  group_by(Q) %>%
  filter(Date == min(Date)) %>%
  mutate(Date = as.Date(paste0(year(Date), "-", sprintf("%02d", month(Date)), "-01")))
drop <- c("Q")
xli_data_clean = xli_data_clean[,!(names(xli_data_clean) %in% drop)]
#xlp_data_clean 
xlp_data_clean$Date<-as.Date(xlp_data_clean$Date,"%m/%d/%Y")
xlp_data_clean <- xlp_data_clean %>%
  mutate(Q = cut.Date(Date, "quarter", labels = FALSE)) %>%
  group_by(Q) %>%
  filter(Date == min(Date)) %>%
  mutate(Date = as.Date(paste0(year(Date), "-", sprintf("%02d", month(Date)), "-01")))
drop <- c("Q")
xlp_data_clean = xlp_data_clean[,!(names(xlp_data_clean) %in% drop)]
#xlk_data_clean
xlk_data_clean$Date<-as.Date(xlk_data_clean$Date,"%m/%d/%Y")
xlk_data_clean <- xlk_data_clean %>%
  mutate(Q = cut.Date(Date, "quarter", labels = FALSE)) %>%
  group_by(Q) %>%
  filter(Date == min(Date)) %>%
  mutate(Date = as.Date(paste0(year(Date), "-", sprintf("%02d", month(Date)), "-01")))
drop <- c("Q")
xlk_data_clean = xlk_data_clean[,!(names(xlk_data_clean) %in% drop)]
#xlre_data_clean 
xlre_data_clean$Date<-as.Date(xlre_data_clean$Date,"%m/%d/%Y")
xlre_data_clean <- xlre_data_clean %>%
  mutate(Q = cut.Date(Date, "quarter", labels = FALSE)) %>%
  group_by(Q) %>%
  filter(Date == min(Date)) %>%
  mutate(Date = as.Date(paste0(year(Date), "-", sprintf("%02d", month(Date)), "-01")))
drop <- c("Q")
xlre_data_clean = xlre_data_clean[,!(names(xlre_data_clean) %in% drop)]
#xlu_data_clean 
xlu_data_clean$Date<-as.Date(xlu_data_clean$Date,"%m/%d/%Y")
xlu_data_clean <- xlu_data_clean %>%
  mutate(Q = cut.Date(Date, "quarter", labels = FALSE)) %>%
  group_by(Q) %>%
  filter(Date == min(Date)) %>%
  mutate(Date = as.Date(paste0(year(Date), "-", sprintf("%02d", month(Date)), "-01")))
drop <- c("Q")
xlu_data_clean = xlu_data_clean[,!(names(xlu_data_clean) %in% drop)]
#xlv_data_clean 
xlv_data_clean$Date<-as.Date(xlv_data_clean$Date,"%m/%d/%Y")
xlv_data_clean <- xlv_data_clean %>%
  mutate(Q = cut.Date(Date, "quarter", labels = FALSE)) %>%
  group_by(Q) %>%
  filter(Date == min(Date)) %>%
  mutate(Date = as.Date(paste0(year(Date), "-", sprintf("%02d", month(Date)), "-01")))
drop <- c("Q")
xlv_data_clean = xlv_data_clean[,!(names(xlv_data_clean) %in% drop)]
#xly_data_clean 
xly_data_clean$Date<-as.Date(xly_data_clean$Date,"%m/%d/%Y")
xly_data_clean <- xly_data_clean %>%
  mutate(Q = cut.Date(Date, "quarter", labels = FALSE)) %>%
  group_by(Q) %>%
  filter(Date == min(Date)) %>%
  mutate(Date = as.Date(paste0(year(Date), "-", sprintf("%02d", month(Date)), "-01")))
drop <- c("Q")
xly_data_clean = xly_data_clean[,!(names(xly_data_clean) %in% drop)]
#interest_data 
interest_data$Date<-as.Date(interest_data$Date,"%m/%d/%Y")
interest_data <- interest_data %>%
  mutate(Q = cut.Date(Date, "quarter", labels = FALSE)) %>%
  group_by(Q) %>%
  filter(Date == min(Date)) %>%
  mutate(Date = as.Date(paste0(year(Date), "-", sprintf("%02d", month(Date)), "-01")))
colnames(interest_data)[2] ="interest"
drop <- c("Q")
interest_data = interest_data[,!(names(interest_data) %in% drop)]
#inflation_data
inflation_data$Date<-as.Date(inflation_data$Date,"%m/%d/%Y")
inflation_data <- inflation_data %>%
  mutate(Q = cut.Date(Date, "quarter", labels = FALSE)) %>%
  group_by(Q) %>%
  filter(Date == min(Date)) %>%
  mutate(Date = as.Date(paste0(year(Date), "-", sprintf("%02d", month(Date)), "-01")))
colnames(inflation_data)[2] ="inflation"
drop <- c("Q")
inflation_data = inflation_data[,!(names(inflation_data) %in% drop)]
#unemployment_data 
unemployment_data$Date<-as.Date(unemployment_data$Date,"%m/%d/%Y")
unemployment_data <- unemployment_data %>%
  mutate(Q = cut.Date(Date, "quarter", labels = FALSE)) %>%
  group_by(Q) %>%
  filter(Date == min(Date)) %>%
  mutate(Date = as.Date(paste0(year(Date), "-", sprintf("%02d", month(Date)), "-01")))
colnames(unemployment_data)[2] ="unemployment"
drop <- c("Q")
unemployment_data = unemployment_data[,!(names(unemployment_data) %in% drop)]
#gdp_data
gdp_data$Date <- as.Date(gdp_data$DATE , "%m/%d/%Y")
#drop DATE column from gdp_data
gdp_data <- gdp_data %>%dplyr::select( Date, value)
colnames(gdp_data)[2] ="gdp"

drop <- c("Q")
df = unemployment_data[,!(names(unemployment_data) %in% drop)]

# final merged data
df_index_factor <- list(dj_data_clean,nasdaq_data_clean, sp500_data_clean,gdp_data, interest_data, inflation_data, unemployment_data)      
index_factor_data <- Reduce(function(x, y) merge(x, y, all=TRUE), df_index_factor) 
index_factor_data <- na.omit(index_factor_data)

df_index_sector <- list(dj_data_clean, nasdaq_data_clean, sp500_data_clean, xlb_data_clean, xlc_data_clean, xle_data_clean, xlf_data_clean, xli_data_clean, xlp_data_clean, xlk_data_clean, xlre_data_clean, xlu_data_clean, xlv_data_clean, xly_data_clean)
index_sector_data <- Reduce(function(x, y) merge(x, y, all=TRUE), df_index_sector) 
index_sector_data <- na.omit(index_sector_data)

df_sector_factor <- list(xlb_data_clean, xlc_data_clean, xle_data_clean, xlf_data_clean, xli_data_clean, xlp_data_clean, xlk_data_clean, xlre_data_clean, xlu_data_clean, xlv_data_clean, xly_data_clean,gdp_data, interest_data, inflation_data, unemployment_data )
sector_factor_data <- Reduce(function(x, y) merge(x, y, all=TRUE), df_sector_factor) 
sector_factor_data <- na.omit(sector_factor_data)

numeric_vars_index_factor_data <- c("DJI.Adjusted", "IXIC.Adjusted", "GSPC.Adjusted", "gdp", "interest", "inflation", "unemployment")
numeric_index_factor_data <- index_factor_data[, numeric_vars_index_factor_data]
normalized_index_factor_data_numeric <- scale(numeric_index_factor_data)
normalized_index_factor_data <- ts(normalized_index_factor_data_numeric, start = c(2010, 1), frequency = 4)
```

Endogenous variable: Dow Jones Index Exogenous variables: Macroeconomic indicators such as GDP, inflation rate, unemployment rate and interest rates

Before proceeding with model selection, it can be useful to examine the feature variables and select the appropriate ones. This can be done by creating cross-correlation function plots, which can serve as a verification of the correlation heatmap for the variables.

##### CCF Plot for Dow Jones and Exogenous Variables

::: panel-tabset
### GDP

```{r}
par(mfrow=c(1,1))
ccf_result <- ccf(normalized_index_factor_data[, c("DJI.Adjusted")], normalized_index_factor_data[, c("gdp")], 
    lag.max = 300,
    main = "Cros-Correlation Plot for Dow Jones Index and GDP Growth Rate ",
    ylab = "CCF")

cat("The sum of cross correlation function is", sum(abs(ccf_result$acf)))
```

### Interest Rate

```{r}
par(mfrow=c(1,1))
ccf_result <- ccf(normalized_index_factor_data[, c("DJI.Adjusted")], normalized_index_factor_data[, c("interest")], 
    lag.max = 300,
    main = "Cros-Correlation Plot for Dow Jones Index and Interest Rate",
    ylab = "CCF")

cat("The sum of cross correlation function is", sum(abs(ccf_result$acf)))
```

### Inflation Rate

```{r}
par(mfrow=c(1,1))
ccf_result <- ccf(normalized_index_factor_data[, c("DJI.Adjusted")], normalized_index_factor_data[, c("inflation")], 
    lag.max = 300,
    main = "Cros-Correlation Plot for Dow Jones Index and Inflation Rate",
    ylab = "CCF")

cat("The sum of cross correlation function is", sum(abs(ccf_result$acf)))
```

### Unemployment Rate

```{r}
par(mfrow=c(1,1))
ccf_result <- ccf(normalized_index_factor_data[, c("DJI.Adjusted")], normalized_index_factor_data[, c("unemployment")], 
    lag.max = 300,
    main = "Cros-Correlation Plot for Dow Jones Index and Unemployment Rate",
    ylab = "CCF")

cat("The sum of cross correlation function is", sum(abs(ccf_result$acf)))
```
:::

The cross-correlation feature plots reaffirm the findings from the heatmap analysis, indicating that inflation and unemployment rate exhibit stronger correlations with the index compared to GDP and interest rates. The cross-correlation plots for GDP and interest rates show weaker and more scattered patterns, with strikes between the blue lines indicating lower correlation coefficients. This suggests that inflation and unemployment rate are more suitable feature variables for the ARIMAXmodel when predicting Dow Jones movements.

Final Exogenous variables: Macroeconomic indicators: Inflation rate and unemployment rate.

##### Endogenous and Exogenous Variables Plot

```{r}
final_dj_factor_data <- index_factor_data %>%dplyr::select( Date,DJI.Adjusted, inflation,unemployment)
numeric_vars_dj_factor_data <- c("DJI.Adjusted", "inflation", "unemployment")
numeric_dj_factor_data <- final_dj_factor_data[, numeric_vars_dj_factor_data]
normalized_dj_factor_data_numeric <- scale(numeric_dj_factor_data)
normalized_dj_factor_data_numeric_df <- data.frame(normalized_dj_factor_data_numeric)
normalized_dj_factor_data_ts <- ts(normalized_dj_factor_data_numeric, start = c(2010, 1), frequency = 4)

autoplot(normalized_dj_factor_data_ts, facets=TRUE) +
  xlab("Year") + ylab("") +
  ggtitle("Dow Jones Index Stock Price, Inflation Rate and Unemployment Rate in USA 2010-2023")
```

##### Check the stationarity

Before proceeding with further analysis, it's important to check the stationarity of the time series data. If the series is not stationary, it needs to be differentiated before performing ARIMAX model. The stationarity of a multivariate time series data can be checked using the Phillips-Perron test, which is specifically designed for multivariate data. By performing the Phillips-Perron test on the multivariate time series data, we can determine if the data is stationary or if it requires differencing to make it suitable for ARIMAX modeling.

```{r}
# Convert your multivariate time series data to a matrix
final_dj_factor_data_ts_multivariate <- as.matrix(normalized_dj_factor_data_ts)

# Check for stationarity using Phillips-Perron test
phillips_perron_test <- ur.pp(final_dj_factor_data_ts_multivariate)  
summary(phillips_perron_test)
```

The Phillips-Perron unit root test output suggests that the data is stationary. The test regression includes an intercept term and tests whether the coefficient on the lagged value is significantly different from one. The estimated coefficients indicate that the intercept is not significant (p-value \> 0.05), while the coefficient on the lagged value is highly significant (p-value \< 2.2e-16), suggesting that the data is stationary.

Additionally, the "Z-alpha" value is the test statistic, which is compared to critical values of the standard normal distribution to determine significance. In this case, the test statistic is -22.979, indicating strong evidence against the null hypothesis of a unit root. The "Z-tau-mu" value is an auxiliary test statistic used to correct for autocorrelation in the residuals, and it is close to zero, suggesting that there is no evidence of residual autocorrelation.

##### Fitting a ARIMAX model

Fitting an ARIMAX model will be a useful approach to modeling and predicting time series data when external factors are believed to have an impact on the response variable. In this case, we will be using an ARIMAX model to predict the DJI Index, taking into account the effects of inflation and unemployment on the Index. By including these exogenous variables in our model, we can improve the accuracy of our predictions and gain a better understanding of the underlying dynamics of the time series.

::: panel-tabset
### Auto ARIMA

```{r}
xreg <- cbind(Inflation = normalized_dj_factor_data_ts[, "inflation"],
              Unemployment = normalized_dj_factor_data_ts[, "unemployment"])

fit <- auto.arima(normalized_dj_factor_data_ts[, "DJI.Adjusted"], xreg = xreg)
summary(fit)
```

### Residuals

```{r}
checkresiduals(fit)
```
:::

The auto.arima function reveals that the best-fit model is (0,1,0). Analysis of the residuals plot indicates that the fluctuates a lot and lies between -2000 to 2000.The ACF plot of the residuals shows significant lags, indicating that the model is performing well. The qq-plot also suggests normality in the residuals. The p-value greater than 0.05 actually suggests that there is no evidence of significant autocorrelation in the residuals, which means that the model is performing well in terms of capturing the temporal patterns in the data.

##### Fitting the model manually

::: panel-tabset
### Linear Model

```{r}
normalized_dj_factor_data_numeric_df$DJI.Adjusted<-ts(normalized_dj_factor_data_numeric_df$DJI.Adjusted,star=decimal_date(as.Date("2010-01-01",format = "%Y-%m-%d")),frequency = 4)
normalized_dj_factor_data_numeric_df$inflation<-ts(normalized_dj_factor_data_numeric_df$inflation,star=decimal_date(as.Date("2010-01-01",format = "%Y-%m-%d")),frequency = 4)
normalized_dj_factor_data_numeric_df$unemployment<-ts(normalized_dj_factor_data_numeric_df$unemployment,star=decimal_date(as.Date("2010-01-01",format = "%Y-%m-%d")),frequency = 4)

############# First fit the linear model##########
fit.reg <- lm(DJI.Adjusted ~ inflation+unemployment, data=normalized_dj_factor_data_numeric_df)
summary(fit.reg)
```

### ACF of Residuals

```{r}
res.fit<-ts(residuals(fit.reg),star=decimal_date(as.Date("2010-01-01",format = "%Y-%m-%d")),frequency = 4)
############## Then look at the residuals ############
acf(res.fit)

```

### ACF of Residuals

```{r}
Pacf(res.fit)
```

### Differentiated Residual

```{r}
res.fit %>% diff() %>% ggtsdisplay()

```
:::

The output displays the results of a linear regression model using two predictor variables, inflation and unemployment, to explain the variation in the response variable, DJI.Adjusted. The coefficients of inflation and unemployment have p-values of less than 0.05, indicating that both variables significantly impact DJI.Adjusted. The R-squared value of approximately 57% suggests that the model explains a moderate amount of the variation in the response variable. Examination of the ACF plot of residuals reveals some autocorrelation, and therefore, the series is differentiated to achieve stationarity. By analyzing the differentiated ACF and PACF plots, the parameters if the ARIMAX model are that p=4, q=1, and d=1.

##### Finding the model parameters.

::: panel-tabset
### ARIMAX Result

```{r}
ARIMA.c=function(p1,p2,q1,q2,data){
temp=c()
d=1
i=1
temp= data.frame()
ls=matrix(rep(NA,6*30),nrow=30)


for (p in p1:p2)#
{
  for(q in q1:q2)#
  {
    for(d in 0:1)
    {
      
      if(p+d+q<=6)
      {
        
        model<- Arima(data,order=c(p,d,q))
        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)
        i=i+1
        #print(i)
        
      }
      
    }
  }
}


temp= as.data.frame(ls)
names(temp)= c("p","d","q","AIC","BIC","AICc")

temp
}

output <- ARIMA.c(1,5,1,5,data=residuals(fit.reg))

output[which.min(output$AIC),] 
output[which.min(output$BIC),]
output[which.min(output$AICc),]
```

### Model 1 Plot

```{r}
set.seed(1234)

model_output_1 <- capture.output(sarima(res.fit, 1,1,3)) 
```

### Model 1

```{r}
cat(model_output_1[45:78], model_output_1[length(model_output_1)], sep = "\n")
```

### Model 2 Plot

```{r}
model_output_2 <- capture.output(sarima(res.fit,0,1,0)) 
```

### Model 2

```{r}
cat(model_output_2[9:38], model_output_2[length(model_output_2)], sep = "\n")
```
:::

After manual fitting, we have identified that the ARIMAX model most suitable for our data is likely (1,1,3). A comparison of its AIC, AICc, and BIC values to those of other models indicates that it has the minimum values. We proceed to fit this model using both manual and auto.arima methods. After fitting, we observe that both models have p-values greater than 0, indicating that they are statistically significant. To determine the best fit model, we use cross-validation.

##### Cross Validation

::: panel-tabset
### RMSE Plot

```{r}
n=length(res.fit)
k= 51
 
 
rmse1 <- matrix(NA, (n-k),4)
rmse2 <- matrix(NA, (n-k),4)


st <- tsp(res.fit)[1]+(k-5)/4 

for(i in 1:(n-k))
{
  xtrain <- window(res.fit, end=st + i/4)
  xtest <- window(res.fit, start=st + (i+1)/4, end=st + (i+4)/4)
  
  #ARIMA(0,1,0) ARIMA(1,1,3)
  
  fit <- Arima(xtrain, order=c(1,1,3),
                include.drift=TRUE, method="ML")
  fcast <- forecast(fit, h=4)
  
  fit2 <- Arima(xtrain, order=c(0,1,0),
                include.drift=TRUE, method="ML")
  fcast2 <- forecast(fit2, h=4)

  

  rmse1[i,1:length(xtest)]   <- sqrt((fcast$mean-xtest)^2)
  rmse2[i,1:length(xtest)] <- sqrt((fcast2$mean-xtest)^2)
  
}

plot(1:4,colMeans(rmse1,na.rm=TRUE), type="l",col=2, xlab="horizon", ylab="RMSE")
lines(1:4, colMeans(rmse2,na.rm=TRUE), type="l",col=3)
legend("topleft",legend=c("fit1","fit2"),col=2:3,lty=1)
```

### RMSE Results

```{r}
cat("RMSE values for Model 1\n", colMeans(rmse1))

cat("RMSE values for Model 2\n", colMeans(rmse2))
```
:::

The RMSE cross-validation plot shows that the RMSE of Model 1 (1,1,3) is lower than that of Model 2 (0,1,0). This suggests that Model 1 (1,1,3) is a better fit for the data when compared to Model 2 (0,1,0).

##### Forecast

::: panel-tabset
### Forecast for DJI with feature variables

```{r}
#fiting an ARIMA model to the Inflation variable
inflation_fit<-auto.arima(normalized_dj_factor_data_numeric_df$inflation) 
finflation<-forecast(inflation_fit)

#fitting an ARIMA model to the Unemployment variable
unemployment_fit<-auto.arima(normalized_dj_factor_data_numeric_df$unemployment) 
funemployment<-forecast(unemployment_fit)

# best model fit for forcasting
xreg <- cbind(Inflation = normalized_dj_factor_data_ts[, "inflation"],
              Unemployment = normalized_dj_factor_data_ts[, "unemployment"])

fit <- Arima(normalized_dj_factor_data_ts[, "DJI.Adjusted"],order=c(1,1,3),xreg=xreg)

# Forcast the stock price using feature variables
fxreg <- cbind(Inflation = finflation$mean,
              Unemployment = funemployment$mean)
fcast <- forecast(fit, xreg=fxreg) 
autoplot(fcast) + xlab("Year") +
  ylab("Normalised DJI.Adjusted")
```

### ARIMA Model for Inflation

```{r}
#fiting an ARIMA model to the Inflation variable
inflation_fit<-auto.arima(normalized_dj_factor_data_numeric_df$inflation) 
finflation<-forecast(inflation_fit)
summary(inflation_fit)
```

### ARIMA Model for Employment

```{r}
#fitting an ARIMA model to the Unemployment variable
unemployment_fit<-auto.arima(normalized_dj_factor_data_numeric_df$unemployment) 
funemployment<-forecast(unemployment_fit)
summary(unemployment_fit)
```

### ARIMA Model for DJI with feature variables

```{r}
# best model fit for forcasting
xreg <- cbind(Inflation = normalized_dj_factor_data_ts[, "inflation"],
              Unemployment = normalized_dj_factor_data_ts[, "unemployment"])

fit <- Arima(normalized_dj_factor_data_ts[, "DJI.Adjusted"],order=c(1,1,3),xreg=xreg)
summary(fit)
```
:::

The ARIMAX model may have incorporated exogenous variables such as macroeconomic factors, which could indicate the potential impact of external factors on the forecasted values. The findings suggest that inflation and unemployment rates significantly impact the DJI stock price, underscoring the importance of including macroeconomic factors in stock market forecasting. This is further supported by the comparison of RMSE, MAE, AIC and BIC values for forecasts with and without the feature variable. It is found that including the feature variable leads to a lower values, indicating that the feature variable makes a significant impact and should be considered in stock market forecasting.

---
editor: visual
format:
  html:
    code-fold: true
    self-contained: true
    page-layout: full
toc: true
---

#### Deep Learning for S&P 500 Index Stock Price

The S&P 500 Index, which is composed of 500 large-cap stocks from different industries, is one of the most widely used benchmarks for the performance of the US stock market. While predicting the future movements of the S&P 500 Index is a challenging task, various models have been developed to forecast its future movements. These models range from traditional time-series models to more advanced deep learning models. By leveraging the power of Deep Learning and applying techniques such as RNN, LSTM, GRU, and regularization, we can create a model that accurately predicts the future movements of the S&P 500 Index. Such a model can provide valuable insights for investors and traders looking to make informed decisions in the stock market, and help them stay ahead of the curve in the ever-changing landscape of the technology and biotech industries.

In order to apply the deep learning methods, the time series data is first split into training and testing sets, taking into account the time component. The training data constitutes 75% of the total data while the rest is the testing data. Furthermore, to ensure that the values are on the same scale, the data is scaled, which is crucial for deep learning models. The mean and standard deviation of the training data are computed and then utilized to normalize both the training and testing sets. This approach ensures that the models are trained on a standardized dataset, enabling them to make accurate predictions.

```{python,echo=FALSE, message=FALSE, warning=FALSE}
#| echo: true
#| message: FALSE
#| warning: FALSE
# libraries
import numpy as np
import pandas as pd
import math
from sklearn.metrics import mean_squared_error
from matplotlib import pyplot as plt
from keras.models import Sequential
from keras.layers import Dropout
from keras.layers import Dense, SimpleRNN, LSTM, GRU
from keras import regularizers
import warnings
warnings.filterwarnings('ignore')

# import data
df = pd.read_csv('./DATA/CLEANED DATA/sp500_raw_data.csv')
# clean data and get final data for anlaysis
df = df[['Date', 'GSPC.Adjusted']]
df['Date'] = pd.to_datetime(df['Date'])
df['GSPC.Adjusted'] = pd.to_numeric(df['GSPC.Adjusted'], errors='coerce')
df = df.dropna(subset=['GSPC.Adjusted'])
df.sort_values('Date', inplace=True, ascending=True)
df = df.reset_index(drop=True)
def create_X_Y(ts: np.array, lag=1, n_ahead=1, target_index=0) -> tuple:
    """
    A method to create X and Y matrix from a time series array for the training of 
    deep learning models 
    """
    # Extracting the number of features that are passed from the array
    n_features = ts.shape[1]

    # Creating placeholder lists
    X, Y = [], []

    if len(ts) - lag <= 0:
        X.append(ts)
    else:
        for i in range(len(ts) - lag - n_ahead):
            Y.append(ts[(i + lag):(i + lag + n_ahead), target_index])
            X.append(ts[i:(i + lag)])

    X, Y = np.array(X), np.array(Y)

    # Reshaping the X array to an RNN input shape
    X = np.reshape(X, (X.shape[0], lag, n_features))

    return X, Y

ts = df[['GSPC.Adjusted']].values
np.random.seed(123)
nrows = ts.shape[0]
test_share = 0.25
# Spliting into train and test sets
train = ts[0:int(nrows * (1 - test_share))]
test = ts[int(nrows * (1 - test_share)):]

# Scaling the data
train_mean = train.mean()
train_std = train.std()
train = (train - train_mean) / train_std
test = (test - train_mean) / train_std

# Creating the final scaled frame
ts_s = np.concatenate([train, test])

lag = 12
ahead = 3

# Creating the X and Y for training
X, Y = create_X_Y(ts_s, lag=lag, n_ahead=ahead)

Xtrain = X[0:int(X.shape[0] * (1 - test_share))]
Ytrain = Y[0:int(X.shape[0] * (1 - test_share))]

Xval = X[int(X.shape[0] * (1 - test_share)):]
Yval = Y[int(X.shape[0] * (1 - test_share)):]




def plot_model(history, model_title):
    loss = history.history['loss']
    epochs = range(1, len(loss) + 1)
    plt.figure()
    plt.plot(epochs, loss, 'b', label='Training loss', color = "gray")
    plt.title(f'{model_title} Training loss ')
    plt.legend()
    


def print_error(trainY, testY, train_predict, test_predict):
    # Error of predictions
    train_rmse = math.sqrt(mean_squared_error(
        trainY[:, 0], train_predict[:, 0]))
    test_rmse = math.sqrt(mean_squared_error(testY[:, 0], test_predict[:, 0]))
    # Print RMSE
    print('Train RMSE: %.3f RMSE' % (train_rmse))
    print('Test RMSE: %.3f RMSE' % (test_rmse))
    return train_rmse, test_rmse
```

#### Recurrent Neural Network

The RNN models that are constructed and trained using TensorFlow and Keras. This model is trained for five hidden layers, one dense layer, and the activation function hyperbolic tangent. A dropout rate of 0.2 is used to reduce the risk of overfitting. The model are trained for 20 epochs and generated validation loss plots to compare the performance of the regularized and non-regularized models. The second RNN model was identical to the first, except for the addition of kernel regularization.

::: panel-tabset
### RNN

```{python , message=FALSE, warning=FALSE}
#| message: FALSE
#| warning: FALSE
# Create a RNN
def create_RNN(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2,kernel_regularizer=None):
    model = Sequential()
    # Create a simple neural network layer
    model.add(SimpleRNN(hidden_units, input_shape=input_shape,
              activation=activation[0]))
    # Add a dense layer (only one, more layers would make it a deep neural net)
    model.add(Dense(units=dense_units,
              activation=activation[1],
              kernel_regularizer=kernel_regularizer))
    # Add layer dropout
    model.add(Dropout(dropout_rate))
    
    # Compile the model and optimize on mean squared error
    model.compile(loss='mean_squared_error', optimizer='adam')
    return model
  
# Create a recurrent neural network
model = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),
                   activation=['tanh', 'tanh'], dropout_rate = 0.2)
history = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0,validation_data=(Xval, Yval))
plot_model(history, 'Recurrent Neural Network Model')

yhat_d = [x[0] for x in model.predict(Xval, verbose=0)]
y = [y[0] for y in Yval]

train_predict = model.predict(Xtrain, verbose=0)
test_predict = model.predict(Xval, verbose=0)

# Print error
train_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)
rmse_table = {
    'model': ['Recurrent Neural Network'],
    'training_rmse': [train_rmse],
    'testing_rmse': [test_rmse]
}

```

### RNN with Regularization

```{python, message=FALSE, warning=FALSE}
#| message: FALSE
#| warning: FALSE
# Create a recurrent neural network with regularization

model = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),
                   activation=['tanh', 'tanh'], kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4), dropout_rate = 0.2)
history = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0, validation_data=(Xval, Yval))
plot_model(history, 'Recurrent Neural Network Model (with L1L2 Regularization)')

yhat_d_reg = [x[0] for x in model.predict(Xval, verbose=0)]

train_predict = model.predict(Xtrain, verbose=0)
test_predict = model.predict(Xval, verbose=0)

# Print error
train_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)
rmse_table['model'].append(
    'Recurrent Neural Network (with L1L2 Regularization)')
rmse_table['training_rmse'].append(train_rmse)
rmse_table['testing_rmse'].append(test_rmse)
```
:::

The second plot illustrates the impact of regularization on the training loss. Compared to the original plot, there are significant differences. The regularization technique had a substantial effect on the testing error of the model. The model trained without regularization had a slightly lower training error of 0.306 RMSE but a considerably higher testing error of 2.478 RMSE. In contrast, the model trained with regularization had a slightly lower training error of 0.304 RMSE, but it produced silghtly high testing error of 2.481 RMSE.

#### GRU Neural Network

A GRU neural network is a powerful machine learning tool widely adopted in various applications. Its key feature is the recurrent gated unit, a type of memory cell that can store information over a data sequence. The GRU neural network is five hidden layers, one dense layer, and the hyperbolic tangent activation function. The model also includes kernel regularization to help address the issue of overfitting. The models are trained for 20 epochs and validation loss plots were generated to compare the performance of the regularized and non-regularized models. Overall, the GRU neural network has shown great promise in accurately predicting future movements in the S&P 500 index.

::: panel-tabset
### GRU

```{python, message=FALSE, warning=FALSE}
#| message: FALSE
#| warning: FALSE
# Create a GRU Neural Network
def create_GRU(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2, kernel_regularizer=None):
    model = Sequential()
    # Create a simple GRU neural network layer
    model.add(GRU(hidden_units, input_shape=input_shape,
              activation=activation[0]))
    # Add a dense layer (only one, more layers would make it a deep neural net)
    model.add(Dense(units=dense_units,
              activation=activation[1], kernel_regularizer=kernel_regularizer))
    # Add layer dropout
    model.add(Dropout(dropout_rate))
    # Compile the model and optimize on mean squared error
    model.compile(loss='mean_squared_error', optimizer='sgd')
    return model

# Training and evaluating a GRU-based model
model = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),
                   activation=['tanh', 'relu'], dropout_rate = 0.2)
history = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)
plot_model(history, 'GRU Model')

yhat_gru = [x[0] for x in model.predict(Xval, verbose=0)]

train_predict = model.predict(Xtrain, verbose=0)
test_predict = model.predict(Xval, verbose=0)


# Print error
train_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)
rmse_table['model'].append('GRU Neural Network')
rmse_table['training_rmse'].append(train_rmse)
rmse_table['testing_rmse'].append(test_rmse)
```

### GRU with Regularization

```{python, message=FALSE, warning=FALSE}
#| message: FALSE
#| warning: FALSE
# Training and evaluating a GRU-based model with regularization
model = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),
                   activation=['tanh', 'relu'],dropout_rate = 0.2,  kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))
history = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)
plot_model(history, 'GRU Model (with L1L2 Regularization)')

yhat_gru_reg = [x[0] for x in model.predict(Xval, verbose=0)]

train_predict = model.predict(Xtrain, verbose=0)
test_predict = model.predict(Xval, verbose=0)

# Print error
train_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)
rmse_table['model'].append('GRU Neural Network (with L1L2 Regularization)')
rmse_table['training_rmse'].append(train_rmse)
rmse_table['testing_rmse'].append(test_rmse)

```
:::

The two plots show the training loss for the GRU model, one with regularization and one without. The plot with regularization is similar to without regularization plot. The model without regularization had a train RMSE of 0.699 and a test RMSE of 1.277. In contrast, the model trained with regularization had a slightly lower training error of 0.696 RMSE, but it produced silghtly high testing error of 1.280 RMSE.

#### LSTM Neural Network

The LSTM (Long Short-Term Memory) neural network is another powerful machine learning tool that has been widely adopted in various applications, particularly in tasks involving sequential data.The LSTM neural network architecture typically consists of multiple LSTM layers, followed by one or more dense layers and an activation function such as the sigmoid or hyperbolic tangent. Similar to the GRU neural network, the LSTM model may also include kernel regularization to address the issue of overfitting. In order to evaluate the performance of the LSTM model, the model can be trained on a dataset for a specified number of epochs, and validation loss plots can be generated to compare the performance of the regularized and non-regularized models.

::: panel-tabset
### LSTM

```{python, message=FALSE, warning=FALSE}
#| message: FALSE
#| warning: FALSE
# Create a LSTM Neural Network
def create_LSTM(hidden_units, dense_units, input_shape, activation,dropout_rate = 0.2, kernel_regularizer=None):
    model = Sequential()
    # Create a simple long short term memory neural network
    model.add(LSTM(hidden_units,
              activation=activation[0], input_shape=input_shape))
    # Add a dense layer (only one, more layers would make it a deep neural net)
    model.add(Dense(units=dense_units,
              activation=activation[1], kernel_regularizer=kernel_regularizer))
    # Add layer dropout
    model.add(Dropout(dropout_rate))
    # Compile the model and optimize on mean squared error
    model.compile(optimizer="RMSprop", loss='mae')
    return model

# Create an LSTM neural network
model = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),
                    activation=['tanh', 'linear'], dropout_rate = 0.2)
history = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)
plot_model(history, 'LSTM Model')

yhat_lstm = [x[0] for x in model.predict(Xval, verbose=0)]

train_predict = model.predict(Xtrain, verbose=0)
test_predict = model.predict(Xval, verbose=0)

# Print error
train_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)
rmse_table['model'].append('LSTM Neural Network')
rmse_table['training_rmse'].append(train_rmse)
rmse_table['testing_rmse'].append(test_rmse)
```

### LSTM with Regularization

```{python, message=FALSE, warning=FALSE}
#| message: FALSE
#| warning: FALSE
# Create an LSTM neural network with regularization
model = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),
                    activation=['tanh', 'linear'], dropout_rate = 0.2, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))
history = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)
plot_model(history, 'LSTM Model (with L1L2 Regularization)')


yhat_lstm_reg = [x[0] for x in model.predict(Xval, verbose=0)]

train_predict = model.predict(Xtrain, verbose=0)
test_predict = model.predict(Xval, verbose=0)

# Print error
train_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)
rmse_table['model'].append('LSTM Neural Network (with L1L2 Regularization)')
rmse_table['training_rmse'].append(train_rmse)
rmse_table['testing_rmse'].append(test_rmse)
```
:::

The two plots show the training loss for the LSTM model, one with regularization and one without. The plot with regularization shows less fluctuation, indicating that overfitting has been reduced. The model without regularization had a train RMSE of 0.169 and a test RMSE of 1.203, while the model with regularization had a slightly higher for both the train and test RMSE values which are 0.202 and 1.389 respectively.

#### Forecast

```{python, message=FALSE, warning=FALSE}
#| message: FALSE
#| warning: FALSE
# Creating the frame to store both predictions
days = df['Date'].values[-len(y):]
frame = pd.concat([
    pd.DataFrame({'day': days, 'price': y, 'type': 'original'}),
    pd.DataFrame({'day': days, 'price': yhat_d, 'type': 'rnn_forecast'}),
    pd.DataFrame({'day': days, 'price': yhat_gru, 'type': 'gru_forecast'}),
    pd.DataFrame({'day': days, 'price': yhat_lstm, 'type': 'lstm_forecast'})
])
# Creating the unscaled values column
frame['price_absolute'] = [(x * train_std) + train_mean for x in frame['price']]
# Pivoting
pivoted = frame.pivot_table(index='day', columns='type')
pivoted.columns = ['_'.join(x).strip() for x in pivoted.columns.values]

plt.figure(figsize=(12, 10))
plt.plot(pivoted.index, pivoted.price_absolute_original,
         color='#8B1874', label='original')
plt.plot(pivoted.index, pivoted.price_absolute_rnn_forecast,
         color='#F79540', label='RNN Forecast', alpha=0.6)
plt.plot(pivoted.index, pivoted.price_absolute_gru_forecast,
         color='#B71375', label='GRU Forecast', alpha=0.6)
plt.plot(pivoted.index, pivoted.price_absolute_lstm_forecast,
         color='#FC4F00', label='LSTM Forecast', alpha=0.6)
plt.title('S&P 500  Index Stock Price Forecasts')
plt.legend()
plt.show()

```

When comparing the predictions of the deep learning models with the original time series plot, we can observe that the LSTM model is the closest to the actual values. The GRU model is almost the same as the LSTM model, but both the models have some kind of accuracy with the actual plot, but there is still difference. RNN model are pretty far from the actual plot, which indicates that LSTM model is better when compared to others, but we need to note that the model is not accurate, i.e., it is not the same as the actual plot.

#### Model Comparison

```{python}
rmse_df = pd.DataFrame(rmse_table)
rmse_df
```

First, we observe that recurrent neural networks (RNN) and long short-term memory (LSTM) networks generally perform better than gated recurrent units (GRU) in terms of testing RMSE. However, the training and testing RMSE values for each model can vary significantly depending on the specific dataset being used.

Regarding regularization, we see that the use of L1L2 regularization generally leads to lower testing RMSE values for RNN and LSTM models. However, for GRU models, the use of L1L2 regularization can either slightly improve or worsen the testing RMSE depending on the dataset.

Overall, the best performing models in terms of testing RMSE across all datasets appear to be the LSTM models with L1L2 regularization, which consistently achieve low testing RMSE values.

#### Comparison of deep learning models with traditional single variable time series

Among the deep learning models, the LSTM Neural Network (with L1L2 Regularization) performed the best with the lowest testing RMSE value. On the other hand, the ARIMA model had the highest testing RMSE value of 48.95665, indicating that it did not perform well in predicting the future movements of the index.

This comparison highlights the advantage of using deep learning models over traditional statistical models such as ARIMA in predicting stock market trends. Deep learning models are better equipped to handle complex and dynamic data, making them more effective in capturing the non-linear relationships and patterns in the stock market data.

Overall, the results suggest that deep learning models can be a valuable tool for investors and analysts in predicting the future movements of the stock market.

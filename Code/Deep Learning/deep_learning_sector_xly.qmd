---
editor: visual
format:
  html:
    code-fold: true
    self-contained: true
    page-layout: full
toc: true
---

#### Deep Learning for Consumer Discretionary Sector Fund

The consumer discretionary sector comprises companies that provide non-essential goods and services, such as clothing, entertainment, and luxury items. These companies tend to be more sensitive to economic cycles and consumer sentiment than companies in other sectors. As with other stock market, predicting the future movements of the Consumer Discretionary Sector Fund can be challenging due to the complex and dynamic nature of the market. These models range from traditional time-series models to more advanced deep learning models. By leveraging the power of Deep Learning and applying techniques such as RNN, LSTM, GRU, and regularization, we can create a model that accurately predicts the future movements of the sector. Such a model can provide valuable insights for investors and traders looking to make informed decisions in the stock market, and help them stay ahead of the curve in the ever-changing landscape of the technology and biotech industries.

In order to apply the deep learning methods, the time series data is first split into training and testing sets, taking into account the time component. The training data constitutes 75% of the total data while the rest is the testing data. Furthermore, to ensure that the values are on the same scale, the data is scaled, which is crucial for deep learning models. The mean and standard deviation of the training data are computed and then utilized to normalize both the training and testing sets. This approach ensures that the models are trained on a standardized dataset, enabling them to make accurate predictions.

```{python,echo=FALSE, message=FALSE, warning=FALSE}
#| echo: true
#| message: FALSE
#| warning: FALSE
# libraries
import numpy as np
import pandas as pd
import math
from sklearn.metrics import mean_squared_error
from matplotlib import pyplot as plt
from keras.models import Sequential
from keras.layers import Dropout
from keras.layers import Dense, SimpleRNN, LSTM, GRU
from keras import regularizers
import warnings
warnings.filterwarnings('ignore')

# import data
df = pd.read_csv('./DATA/CLEANED DATA/XLY_raw_data.csv')
# clean data and get final data for anlaysis
df = df[['Date', 'XLY.Adjusted']]
df['Date'] = pd.to_datetime(df['Date'])
df['XLY.Adjusted'] = pd.to_numeric(df['XLY.Adjusted'], errors='coerce')
df = df.dropna(subset=['XLY.Adjusted'])
df.sort_values('Date', inplace=True, ascending=True)
df = df.reset_index(drop=True)
def create_X_Y(ts: np.array, lag=1, n_ahead=1, target_index=0) -> tuple:
    """
    A method to create X and Y matrix from a time series array for the training of 
    deep learning models 
    """
    # Extracting the number of features that are passed from the array
    n_features = ts.shape[1]

    # Creating placeholder lists
    X, Y = [], []

    if len(ts) - lag <= 0:
        X.append(ts)
    else:
        for i in range(len(ts) - lag - n_ahead):
            Y.append(ts[(i + lag):(i + lag + n_ahead), target_index])
            X.append(ts[i:(i + lag)])

    X, Y = np.array(X), np.array(Y)

    # Reshaping the X array to an RNN input shape
    X = np.reshape(X, (X.shape[0], lag, n_features))

    return X, Y

ts = df[['XLY.Adjusted']].values
np.random.seed(123)
nrows = ts.shape[0]
test_share = 0.25
# Spliting into train and test sets
train = ts[0:int(nrows * (1 - test_share))]
test = ts[int(nrows * (1 - test_share)):]

# Scaling the data
train_mean = train.mean()
train_std = train.std()
train = (train - train_mean) / train_std
test = (test - train_mean) / train_std

# Creating the final scaled frame
ts_s = np.concatenate([train, test])

lag = 12
ahead = 3

# Creating the X and Y for training
X, Y = create_X_Y(ts_s, lag=lag, n_ahead=ahead)

Xtrain = X[0:int(X.shape[0] * (1 - test_share))]
Ytrain = Y[0:int(X.shape[0] * (1 - test_share))]

Xval = X[int(X.shape[0] * (1 - test_share)):]
Yval = Y[int(X.shape[0] * (1 - test_share)):]




def plot_model(history, model_title):
    loss = history.history['loss']
    epochs = range(1, len(loss) + 1)
    plt.figure()
    plt.plot(epochs, loss, 'b', label='Training loss', color = "gray")
    plt.title(f'{model_title} Training loss ')
    plt.legend()
    


def print_error(trainY, testY, train_predict, test_predict):
    # Error of predictions
    train_rmse = math.sqrt(mean_squared_error(
        trainY[:, 0], train_predict[:, 0]))
    test_rmse = math.sqrt(mean_squared_error(testY[:, 0], test_predict[:, 0]))
    # Print RMSE
    print('Train RMSE: %.3f RMSE' % (train_rmse))
    print('Test RMSE: %.3f RMSE' % (test_rmse))
    return train_rmse, test_rmse
```

#### Recurrent Neural Network

The RNN models that are constructed and trained using TensorFlow and Keras. This model is trained for five hidden layers, one dense layer, and the activation function hyperbolic tangent. A dropout rate of 0.2 is used to reduce the risk of overfitting. The model are trained for 20 epochs and generated validation loss plots to compare the performance of the regularized and non-regularized models. The second RNN model was identical to the first, except for the addition of kernel regularization.

::: panel-tabset
### RNN

```{python , message=FALSE, warning=FALSE}
#| message: FALSE
#| warning: FALSE
# Create a RNN
def create_RNN(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2,kernel_regularizer=None):
    model = Sequential()
    # Create a simple neural network layer
    model.add(SimpleRNN(hidden_units, input_shape=input_shape,
              activation=activation[0]))
    # Add a dense layer (only one, more layers would make it a deep neural net)
    model.add(Dense(units=dense_units,
              activation=activation[1],
              kernel_regularizer=kernel_regularizer))
    # Add layer dropout
    model.add(Dropout(dropout_rate))
    
    # Compile the model and optimize on mean squared error
    model.compile(loss='mean_squared_error', optimizer='adam')
    return model
  
# Create a recurrent neural network
model = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),
                   activation=['tanh', 'tanh'], dropout_rate = 0.2)
history = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0,validation_data=(Xval, Yval))
plot_model(history, 'Recurrent Neural Network Model')

yhat_d = [x[0] for x in model.predict(Xval, verbose=0)]
y = [y[0] for y in Yval]

train_predict = model.predict(Xtrain, verbose=0)
test_predict = model.predict(Xval, verbose=0)

# Print error
train_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)
rmse_table = {
    'model': ['Recurrent Neural Network'],
    'training_rmse': [train_rmse],
    'testing_rmse': [test_rmse]
}
```

### RNN with Regularization

```{python, message=FALSE, warning=FALSE}
#| message: FALSE
#| warning: FALSE
# Create a recurrent neural network with regularization

model = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),
                   activation=['tanh', 'tanh'], kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4), dropout_rate = 0.2)
history = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0, validation_data=(Xval, Yval))
plot_model(history, 'Recurrent Neural Network Model (with L1L2 Regularization)')

yhat_d_reg = [x[0] for x in model.predict(Xval, verbose=0)]

train_predict = model.predict(Xtrain, verbose=0)
test_predict = model.predict(Xval, verbose=0)

# Print error
train_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)
rmse_table['model'].append(
    'Recurrent Neural Network (with L1L2 Regularization)')
rmse_table['training_rmse'].append(train_rmse)
rmse_table['testing_rmse'].append(test_rmse)

```
:::

The second plot illustrates the impact of regularization on the training loss. Compared to the original plot, there are significant differences. The regularization technique had a substantial effect on the testing error of the model. The model trained without regularization had a slightly higher training error of 0.317 RMSE and testing error of 2.482 RMSE when compared to with regularization.

#### GRU Neural Network

A GRU neural network is a powerful machine learning tool widely adopted in various applications. Its key feature is the recurrent gated unit, a type of memory cell that can store information over a data sequence. The GRU neural network is five hidden layers, one dense layer, and the hyperbolic tangent activation function. The model also includes kernel regularization to help address the issue of overfitting. The models are trained for 20 epochs and validation loss plots were generated to compare the performance of the regularized and non-regularized models. Overall, the GRU neural network has shown great promise in accurately predicting future movements in the sector.

::: panel-tabset
### GRU

```{python, message=FALSE, warning=FALSE}
#| message: FALSE
#| warning: FALSE
# Create a GRU Neural Network
def create_GRU(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2, kernel_regularizer=None):
    model = Sequential()
    # Create a simple GRU neural network layer
    model.add(GRU(hidden_units, input_shape=input_shape,
              activation=activation[0]))
    # Add a dense layer (only one, more layers would make it a deep neural net)
    model.add(Dense(units=dense_units,
              activation=activation[1], kernel_regularizer=kernel_regularizer))
    # Add layer dropout
    model.add(Dropout(dropout_rate))
    # Compile the model and optimize on mean squared error
    model.compile(loss='mean_squared_error', optimizer='sgd')
    return model

# Training and evaluating a GRU-based model
model = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),
                   activation=['tanh', 'relu'], dropout_rate = 0.2)
history = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)
plot_model(history, 'GRU Model')

yhat_gru = [x[0] for x in model.predict(Xval, verbose=0)]

train_predict = model.predict(Xtrain, verbose=0)
test_predict = model.predict(Xval, verbose=0)

# Print error
train_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)
rmse_table['model'].append('GRU Neural Network')
rmse_table['training_rmse'].append(train_rmse)
rmse_table['testing_rmse'].append(test_rmse)
```

### GRU with Regularization

```{python, message=FALSE, warning=FALSE}
#| message: FALSE
#| warning: FALSE
# Training and evaluating a GRU-based model with regularization
model = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),
                   activation=['tanh', 'relu'],dropout_rate = 0.2,  kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))
history = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)
plot_model(history, 'GRU Model (with L1L2 Regularization)')

yhat_gru_reg = [x[0] for x in model.predict(Xval, verbose=0)]

train_predict = model.predict(Xtrain, verbose=0)
test_predict = model.predict(Xval, verbose=0)

# Print error
train_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)
rmse_table['model'].append('GRU Neural Network (with L1L2 Regularization)')
rmse_table['training_rmse'].append(train_rmse)
rmse_table['testing_rmse'].append(test_rmse)

```
:::

The two plots show the training loss for the GRU model, one with regularization and one without. The plot with regularization is similar to without regularization plot. The model without regularization had a train RMSE of 0.998 and a test RMSE of 3.424, the values are the higher when compared to with regularization.

#### LSTM Neural Network

The LSTM (Long Short-Term Memory) neural network is another powerful machine learning tool that has been widely adopted in various applications, particularly in tasks involving sequential data.The LSTM neural network architecture typically consists of multiple LSTM layers, followed by one or more dense layers and an activation function such as the sigmoid or hyperbolic tangent. Similar to the GRU neural network, the LSTM model may also include kernel regularization to address the issue of overfitting. In order to evaluate the performance of the LSTM model, the model can be trained on a dataset for a specified number of epochs, and validation loss plots can be generated to compare the performance of the regularized and non-regularized models.

::: panel-tabset
### LSTM

```{python, message=FALSE, warning=FALSE}
#| message: FALSE
#| warning: FALSE
# Create a LSTM Neural Network
def create_LSTM(hidden_units, dense_units, input_shape, activation,dropout_rate = 0.2, kernel_regularizer=None):
    model = Sequential()
    # Create a simple long short term memory neural network
    model.add(LSTM(hidden_units,
              activation=activation[0], input_shape=input_shape))
    # Add a dense layer (only one, more layers would make it a deep neural net)
    model.add(Dense(units=dense_units,
              activation=activation[1], kernel_regularizer=kernel_regularizer))
    # Add layer dropout
    model.add(Dropout(dropout_rate))
    # Compile the model and optimize on mean squared error
    model.compile(optimizer="RMSprop", loss='mae')
    return model

# Create an LSTM neural network
model = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),
                    activation=['tanh', 'linear'], dropout_rate = 0.2)
history = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)
plot_model(history, 'LSTM Model')

yhat_lstm = [x[0] for x in model.predict(Xval, verbose=0)]

train_predict = model.predict(Xtrain, verbose=0)
test_predict = model.predict(Xval, verbose=0)

# Print error
train_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)
rmse_table['model'].append('LSTM Neural Network')
rmse_table['training_rmse'].append(train_rmse)
rmse_table['testing_rmse'].append(test_rmse)
```

### LSTM with Regularization

```{python, message=FALSE, warning=FALSE}
#| message: FALSE
#| warning: FALSE
# Create an LSTM neural network with regularization
model = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),
                    activation=['tanh', 'linear'], dropout_rate = 0.2, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))
history = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)
plot_model(history, 'LSTM Model (with L1L2 Regularization)')


yhat_lstm_reg = [x[0] for x in model.predict(Xval, verbose=0)]

train_predict = model.predict(Xtrain, verbose=0)
test_predict = model.predict(Xval, verbose=0)

# Print error
train_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)
rmse_table['model'].append('LSTM Neural Network (with L1L2 Regularization)')
rmse_table['training_rmse'].append(train_rmse)
rmse_table['testing_rmse'].append(test_rmse)

```
:::

The two plots show the training loss for the LSTM model, one with regularization and one without. The plot with regularization shows less fluctuation, indicating that overfitting has been reduced. The model without regularization had a train RMSE of 0.197 and a test RMSE of 1.321, while the model with regularization had a slightly lower for both the train and test RMSE values which are 0.193 and 1.152 respectively.

#### Forecast

```{python, message=FALSE, warning=FALSE}
#| message: FALSE
#| warning: FALSE
# Creating the frame to store both predictions
days = df['Date'].values[-len(y):]
frame = pd.concat([
    pd.DataFrame({'day': days, 'price': y, 'type': 'original'}),
    pd.DataFrame({'day': days, 'price': yhat_d, 'type': 'rnn_forecast'}),
    pd.DataFrame({'day': days, 'price': yhat_gru, 'type': 'gru_forecast'}),
    pd.DataFrame({'day': days, 'price': yhat_lstm, 'type': 'lstm_forecast'})
])
# Creating the unscaled values column
frame['price_absolute'] = [(x * train_std) + train_mean for x in frame['price']]
# Pivoting
pivoted = frame.pivot_table(index='day', columns='type')
pivoted.columns = ['_'.join(x).strip() for x in pivoted.columns.values]

plt.figure(figsize=(12, 10))
plt.plot(pivoted.index, pivoted.price_absolute_original,
         color='#8B1874', label='original')
plt.plot(pivoted.index, pivoted.price_absolute_rnn_forecast,
         color='#F79540', label='RNN Forecast', alpha=0.6)
plt.plot(pivoted.index, pivoted.price_absolute_gru_forecast,
         color='#B71375', label='GRU Forecast', alpha=0.6)
plt.plot(pivoted.index, pivoted.price_absolute_lstm_forecast,
         color='#FC4F00', label='LSTM Forecast', alpha=0.6)
plt.title('Consumer Discretionary Sector Fund Forecasts')
plt.legend()
plt.show()

```

When comparing the predictions of the deep learning models with the original time series plot, we can observe that the LSTM model is the closest to the actual values. The GRU model and RNN model are pretty far from the actual plot, which indicates that LSTM model is better when compared to others, but when we look at the LSTM model, the model is not accurate, i.e., it is not the same as the actual plot.

#### Model Comparison

```{python}
rmse_df = pd.DataFrame(rmse_table)
rmse_df
```


Comparing the models based on their training and testing root mean square error (RMSE) values, the performance of the models varied across the different datasets. In general, the models with L1L2 regularization tended to perform slightly better on the testing set, indicating that the regularization helped to prevent overfitting. Overall, the LSTM models tended to perform the best on these datasets, particularly when regularization was not used.

#### Comparison of deep learning models with traditional single variable time series

The deep learning models (RNN, GRU, LSTM) perform better than the ARIMA model in both training and testing RMSE values for the Consumer Discretionary Sector Fund. Among the deep learning models, the LSTM model without regularization has the lowest testing RMSE value of 0.709320, when compared to ARIMA Model which is 2.118625, indicating its better performance in predicting the market prices of the Consumer Discretionary Sector Fund compared to the other deep learning models and ARIMA model.

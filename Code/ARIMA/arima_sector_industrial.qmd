---
title: "ARIMA Model for Industrial Sector Fund"
editor: visual
format:
  html:
    code-fold: true
    self-contained: true
    page-layout: full
---

```{r ,echo=FALSE, message=FALSE, warning=FALSE}
library(flipbookr)
library(tidyverse)
library(ggplot2)
library(forecast)
library(astsa) 
library(xts)
library(tseries)
library(fpp2)
library(fma)
library(lubridate)
library(tidyverse)
library(TSstudio)
library(quantmod)
library(tidyquant)
library(plotly)
library(ggplot2)
require(gridExtra)
options(dplyr.summarise.inform = FALSE)
```

During exploratory data analysis (EDA), it was observed that the raw data is often non-stationary, exhibiting trends, seasonality, and other complex patterns that can make modeling and forecasting difficult. To address this issue, the data is often differentiated to achieve stationarity, which is a key assumption of the ARIMA model.

Differencing involves calculating the difference between each data point and the previous data point, which can help to remove trends and seasonality from the data. The goal is to create a new series of data that is stationary, meaning that the statistical properties of the data do not change over time.

#### Differentiated Time Series Plot

::: panel-tabset
##### Plot

```{r warning=FALSE}
#import the data
df <- read.csv("DATA/CLEANED DATA/XLI_raw_data.csv")
#convert to time series data
myts<-ts(df$XLI.Adjusted,frequency=252,start=c(2010,1,1), end = c(2023,3,1)) 
#First order differentiation
df1 <- diff(myts)
# Plot 
myts  %>% diff() %>% ggtsdisplay(main = "First order differentiation") 
```

##### ACF

```{r warning=FALSE}
#ACF plot 
ggAcf(df1,60,main="ACF Plot: First order differentiation") 
```

##### PACF

```{r warning=FALSE}
ggPacf(df1,60,main="PACF Plot: First order differentiation") 
```

##### ADF Test

```{r warning=FALSE}
tseries::adf.test(df1)
```
:::

After analyzing the ACF and PACF plots, it can be observed that most of the bar lines lie between the blue lines, indicating that the time series is stationary. This has been further confirmed through an Augmented Dickey-Fuller test as the p-value is less than 0.05. Once the data is stationary, the next step is to model it using ARIMA. The combination of ACF and PACF plots can help determine the appropriate values for p, d, and q in the ARIMA model. The order of differencing required to achieve stationarity gives the d value. The p value is determined by examining significant spikes at various lags in the PACF plot, while the q value is determined through significant spikes in the ACF plot. It is important to consider other methods such as grid search and information criteria to ensure the most appropriate model is selected.

Here the parameters are d = 1 p = 0 (PACF Plot) q = 0 (ACF Plot)

#### Model Selection

::: panel-tabset
##### ARIMA Result

```{r warning=FALSE}
Arima(myts,order=c(0,1,0),include.drift=TRUE) 
```

##### Auto Arima Model

```{r warning=FALSE}
auto.arima(myts)
```
:::

In the Model selection, chosen model is the same as the auto.arima generated model. We can proceed with the further model diagnostic for the chosen best model i.e. (0,1,0), to look into the details of the model.

#### Model Diagnostic

::: panel-tabset
##### Model Plot

```{r warning=FALSE}
model_output <- capture.output(sarima(myts,0,1,0))

```

##### Model

```{r warning=FALSE}
cat(model_output[9:38], model_output[length(model_output)], sep = "\n") 
```
:::

The model diagnostic is done for ARIMA(0,1,0). By analyzing the standardized residuals plot, it can be observed that the mean is close to 0 and the variance is slightly higher than 1. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows very few significant lags, which is a positive sign for the model. The qq-plot also suggests normality in the residuals. Additionally, the p-values for the Ljung-Box test are greater than 0.05, indicating that the residuals are independent, which is favorable for the model's accuracy.

The equation for this model: $$(1-\phi_1B-\phi_2B^2)(1-B)(Y_t-\mu) = (1+\theta_1B+\theta_2B^2)\epsilon_t$$

#### Forcast

::: panel-tabset
##### Short Term Forcast

```{r warning=FALSE}
myts %>%
  Arima(order=c(0,1,0),include.drift = TRUE) %>%
  forecast %>%
  autoplot(main = "Industrial Sector Fund Stock Prices Prediction") +
  ylab("stock prices") + xlab("Year")
```

##### Long term Forcast

```{r warning=FALSE}
sarima.for(myts,182, 0,1,0, main='Industrial Sector Fund Stock Prices Prediction')
```
:::

The two figures above show an idea of the short term and the long-term forecasted stock price. The forecasted values is be based on the estimated parameters of the model, which were obtained by fitting it to the historical data. One way to assess the accuracy of the forecasts is to use cross-validation, which involves splitting the data into training and testing sets and comparing the predicted values to the actual values in the test set. This can help to identify any potential problems with the model and to fine-tune the model parameters.

::: panel-tabset
##### 12 Step ahead Cross Validation

```{r warning=FALSE}
#a seasonal cross validation using 12 steps ahead forecasts
farima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)), h=h)}

# Compute cross-validated errors for up to 12 steps ahead
e <- tsCV(myts, forecastfunction = farima1, h = 12)

# Compute the MSE values and remove missing values
mse <- colMeans(e^2, na.rm = TRUE)

# Plot the MSE values against the forecast horizon
data.frame(h = 1:12, MSE = mse) %>%
  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle("12 Step Ahead Cross Validation")
```

##### 1 Step ahead Cross Validation

```{r warning=FALSE}
#a seasonal cross validation using 1 steps ahead forecasts
farima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)),h=h)}

# Compute cross-validated errors for up to 1 steps ahead
e <- tsCV(myts, forecastfunction = farima1, h = 1)
mse <-abs(mean(e,na.rm=TRUE))

# Plot the MSE values against the forecast horizon
data.frame(h = 1:12, MSE = mse) %>%
  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle("1 Step Ahead Cross Validation")
```
:::

The one-step-ahead forecasting plot shows that the MSE seems to be stable. The MSE is tend to rise by 12 steps at each step of cross-validation. Lastly, the above-mentioned ARIMA model should be compared to benchmark methods before being used for a long time.

#### Model Comparison

::: panel-tabset
##### Plot

```{r}
fit <- Arima(myts, order=c(0,1,0))
autoplot(myts) +
  autolayer(meanf(myts, h=182),
            series="Mean", PI=FALSE) +
  autolayer(naive(myts, h=182),
            series="Naïve", PI=FALSE) +
  autolayer(snaive(myts, h=182),
            series="SNaïve", PI=FALSE)+
  autolayer(rwf(myts, h=182, drift=TRUE),
            series="Drift", PI=FALSE)+
  autolayer(forecast(fit,182), 
            series="fit",PI=FALSE) +
  guides(colour=guide_legend(title="Forecast"))
```

##### Model Error Table

```{r}
summary <- summary(fit)
snaive <- snaive(myts, h=182)
accuracy(snaive)
summary
```

| Error |        Model |    Snavie |
|:------|-------------:|----------:|
| ME    |  0.004904666 |  1.759821 |
| RMSE  |     1.317807 |  23.47883 |
| MAE   |    0.4339241 |  14.15936 |
| MPE   |  -0.03607689 | -9.529885 |
| MAPE  |     0.931558 |  38.60253 |
| MASE  |   0.03064573 | 1.0000000 |
| ACF1  | -0.006843027 | 0.9965928 |
:::

The ARIMA fitted forecast and snaive tracks the actual data points very closely when compared to other model. The other forecast methods are less accurate when compared. From the table, Model error measurements of fit are much lower than snaive method.We can conclude that the fitted model is good.
